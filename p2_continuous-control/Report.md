# üìÑ Report.md ‚Äì Continuous Control (Udacity DRL Nanodegree)

## üìå 1. Project Overview

This project uses **Deep Deterministic Policy Gradient (DDPG)** to solve the Unity Reacher environment. The objective is to train a robotic arm to control its hand and keep it within a moving target area in a continuous control space.

üéØ **Goal:** Achieve an average score of **+30 over 100 consecutive episodes**.

‚úÖ **Reward:** The agent receives **+0.1** for every timestep its hand remains within the target zone.

‚è±Ô∏è **Episode End:** The episode ends after 1000 timesteps or earlier if the simulation ends.

‚ùå **No penalties** are applied; the agent simply stops receiving rewards when it misses the target.

---

## üß† 2. Learning Algorithm: Deep Deterministic Policy Gradient (DDPG)

### üîß Key Features
- Actor-Critic architecture  
- Experience Replay Buffer  
- Target networks with soft updates  
- Ornstein-Uhlenbeck noise for exploration in continuous spaces  

### üî¢ Hyperparameters

| Parameter         | Value     |
|------------------|-----------|
| Replay Buffer     | 1e6       |
| Batch Size        | 128       |
| Gamma (Œ≥)         | 0.99      |
| Tau (œÑ)           | 1e-3      |
| Actor LR          | 1e-4      |
| Critic LR         | 1e-3      |
| Weight Decay      | 0         |
| Update Every      | 20 steps  |
| Updates per Step  | 5         |
| Noise Sigma       | 0.2       |

---

## üß© 3. Neural Network Architecture

Defined in `model.py`:

### Actor Network
- **Input:** 33-dimensional state  
- **Hidden Layers:**  
  - Linear(33 ‚Üí 256) ‚Üí LayerNorm ‚Üí ReLU  
  - Linear(256 ‚Üí 256) ‚Üí LayerNorm ‚Üí ReLU  
- **Output:** Linear(256 ‚Üí 4) ‚Üí Tanh

### Critic Network
- **Input:** 33-dimensional state + 4-dimensional action  
- **Hidden Layers:**  
  - Linear(33 ‚Üí 256) ‚Üí LayerNorm ‚Üí ReLU  
  - Concatenate 4-dim action ‚Üí 260-dim  
  - Linear(260 ‚Üí 256) ‚Üí LayerNorm ‚Üí ReLU  
- **Output:** Linear(256 ‚Üí 1) ‚Üí Q-value

---

## üìà 4. Performance

The agent successfully solved the environment:

| Episode | Average Score |
|---------|----------------|
| 100     | 4.66           |
| 150     | 10.39          |
| 200     | 17.32          |
| 250     | 26.31          |
| 274     | 30.03 ‚úÖ        |

> **Environment solved in 274 episodes!**

## üìä 5  Training Progress
![Training Progress](img/output.png)
---


## üìÅ 6. Files Included

| File                    | Description                         |
|-------------------------|-------------------------------------|
| `Continuous_Control.ipynb` | Training script and environment logic |
| `ddpg_agent.py`         | DDPG agent implementation           |
| `model.py`              | Actor and Critic network definitions |
| `checkpoint_actor.pth`  | Saved Actor weights                 |
| `checkpoint_critic.pth` | Saved Critic weights                |
| `README.md`             | Setup and execution instructions    |
| `report.md`             | This report                         |

---

## üî≠ 7. Future Work

- Add support for multi-agent environments  
- Use Prioritized Experience Replay  
- Integrate Distributional Critics or Noisy Networks  
- Explore adaptive exploration methods (e.g., parameter noise)  
- Optimize network depth and activation functions  

---
